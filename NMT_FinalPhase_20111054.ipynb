{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NMT-FinalPhase-20111054.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2luzLcju6kCk"
      },
      "source": [
        "#**Hindi to English Seq2Seq Neural Machine translation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-1WxSJN6nNR"
      },
      "source": [
        "## Specifications:\n",
        "1. Architecture: Seq2seq with RNN:\n",
        "    Encoder: Bidirectional single layer GRU\n",
        "    Decoder: Unidirectional single layer GRU with     attention mechanism\n",
        "2. Vocabulary size: 37208 for Hindi, 28157 for English\n",
        "3. Tokenizer: NLTK TweetTokenizer for English, IndicNLP trivial tokenizer for Hindi\n",
        "4. Encoder: Input size: 37208, Embedding size: 512, Dropout: 0.5\n",
        "5. Decoder: Input size: 28157, Embedding size: 512, Dropout: 0.5\n",
        "6. Output size: 28157, Hidden size: 512\n",
        "7. Layers: 1\n",
        "8. Epochs: 50 (Best model at 45 epoch), Batch size: 64\n",
        "9. Learning rate: 0.001\n",
        "10. Optimizer: AdamW, Loss: Cross Entropy Loss\n",
        "\n",
        "## How to run?\n",
        "train.csv is optional, but cleaned-train-randomized.csv should be mounted (provided in folder), because it's a randomized version of the train set, and if this file is generated again, the vocabulary for the trained model will not match. Required packages will be installed and other files will be automatically generated by the notebook. testhindistatements.csv should also be mounted for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p0dJUwy6q_k"
      },
      "source": [
        "## Installing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DARCiJ20BJni"
      },
      "source": [
        "The packages we're going to use except the python basics are indic_nlp, torch and nltk. indic_nlp needs to be built from source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdNKAlVFOI2L",
        "outputId": "a0321fee-96df-40de-a6fd-919792243c2a"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1271, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 16.12 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 41.31 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px6A19iZu1Qz",
        "outputId": "d8de5b5f-2a90-441e-d722-180d5fd03be1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqKDXHDgOYHf"
      },
      "source": [
        "# Setting path for INDIC_NLP_LIBRARY\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgO9xpAM_Iij"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "import csv\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "import torch.nn.functional as func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9X1Ipae6wX3"
      },
      "source": [
        "**Sentence tokenization functions:** Using NLTK TweetTokenizer for tokenizing English and indic_nlp for tokenizing Hindi sentences. TweetTokenizer is chosen because it does not split on apostrophe, letting us preserve words like don't, can't etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eked1HVBtxD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b789e420-51dd-4140-8323-f36b8a3af59d"
      },
      "source": [
        "def tokenize_hindi(text_in_hindi): # Tokenize hindi sentences\n",
        "    hindi_tokens=[]\n",
        "    for token in indic_tokenize.trivial_tokenize(text_in_hindi):\n",
        "        hindi_tokens.append(token)\n",
        "    return hindi_tokens \n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "def tokenize_english(eng_text): # Tokenize english sentences\n",
        "     return [token for token in tokenizer.tokenize(eng_text)]\n",
        "print(tokenize_english(\"i  can't help you in doing that.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', \"can't\", 'help', 'you', 'in', 'doing', 'that', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HqGR-rn6ydC"
      },
      "source": [
        "## Data Cleaning and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TSlEKpk6z4b"
      },
      "source": [
        "1. English text is converted to lower case. \n",
        "2. Extra spaces are removed.\n",
        "3. All special characters except . ? ! are removed. Multiple quotations, eg \"\"\" are converted to single quotations \". Multiple occurances of . eg '...' are converted to a single occurance '.'\n",
        "4. The character '.' is removed if found at any position except the end of an english sentence.\n",
        "4. Hindi numbers are converted to their english versions.\n",
        "5. Consistency of ending punctuations have been maintained. eg If the hindi sentence ends with |, English sentence must end with . and if the hindi sentence ends with ? the english sentence must also end with ? and so on.\n",
        "6. We take only the sentences containing <=50 words from train.csv. This preserves 78k out of 80k sentences in the dataset, but gives lesser <pad> overhead for GPU and faster runtime. The gain transcends the loss of 2k sentences.\n",
        "7. Hindi sentences containing english words are dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATE9W_2ROaQ8"
      },
      "source": [
        "dic={}\n",
        "with open('cleaned_train.csv', 'w', newline='\\n',encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    csv_file = open(\"train.csv\",encoding='utf-8')\n",
        "    rows = csv.reader(csv_file)\n",
        "    for row in rows:\n",
        "        hindi = str(row[1]).lower().strip()\n",
        "        english = str(row[2]).lower().strip() # Convert to lower case\n",
        "\n",
        "        english = english.replace('’','\\'')\n",
        "        hindi = hindi.replace('’','\\'')\n",
        "        hindi=re.sub(r'[/\\-:;%()♪♫<>,~¶#&=]+','',hindi)\n",
        "        english=re.sub(r\"[^a-z0-9!.?'\\[\\] ]+\",'',english)\n",
        "        english=re.sub('\"+','', english)\n",
        "        hindi=re.sub('\"+','', hindi)\n",
        "        english=re.sub(' +',' ', english)\n",
        "        hindi=re.sub(' +',' ', hindi)\n",
        "        # Removed punctuations and extra space\n",
        "\n",
        "        hindi = hindi.replace(\"०\", \"0\")\n",
        "        hindi = hindi.replace(\"१\", \"1\")\n",
        "        hindi = hindi.replace(\"२\", \"2\")\n",
        "        hindi = hindi.replace(\"३\", \"3\")\n",
        "        hindi = hindi.replace(\"४\", \"4\")\n",
        "        hindi = hindi.replace(\"५\", \"5\")\n",
        "        hindi = hindi.replace(\"६\", \"6\")\n",
        "        hindi = hindi.replace(\"७\", \"7\")\n",
        "        hindi = hindi.replace(\"८\", \"8\")\n",
        "        hindi = hindi.replace(\"९\", \"9\")\n",
        "        # Replace hindi digits with english digits\n",
        "\n",
        "        if len(english)>0 and len(hindi)>0 and english[-1]=='.' and hindi[-1]!='।' and hindi[-1]!='.' and hindi[-1]!='!' and hindi[-1]!='?' and hindi[-1]!='\"' and hindi[-1]!='|':\n",
        "            hindi+='।'\n",
        "        elif len(english)>0 and len(hindi)>0 and english[-1]=='?' and hindi[-1]!='।' and hindi[-1]!='.' and hindi[-1]!='!' and hindi[-1]!='?' and hindi[-1]!='\"' and hindi[-1]!='|':\n",
        "            hindi+='?'\n",
        "        elif len(english)>0 and len(hindi)>0 and english[-1]=='!' and hindi[-1]!='।' and hindi[-1]!='.' and hindi[-1]!='!' and hindi[-1]!='?' and hindi[-1]!='\"' and hindi[-1]!='|':\n",
        "            hindi+='!'\n",
        "        if len(english)>0 and len(hindi)>0 and english[-1]!='.' and english[-1]!='!' and english[-1]!='?' and hindi[-1]=='।':\n",
        "            english+='.'\n",
        "        elif len(english)>0 and len(hindi)>0 and english[-1]!='.' and english[-1]!='!' and english[-1]!='?' and hindi[-1]=='?':\n",
        "            english+='?'\n",
        "        elif len(english)>0 and len(hindi)>0 and english[-1]!='.' and english[-1]!='!' and english[-1]!='?' and hindi[-1]=='!':\n",
        "            english+='!'\n",
        "        # Insert stop words to maintain consistency\n",
        "\n",
        "        if(\"...\" in hindi and hindi.index(\"...\")!=len(hindi)-3):\n",
        "            hindi = hindi.replace(\"...\",\" \")\n",
        "        if(\"...\" in english and english.index(\"...\")!=len(english)-3):\n",
        "            english = english.replace(\"...\",\" \")\n",
        "        hindi = hindi.replace(\"....\",\".\")\n",
        "        hindi = hindi.replace(\"...\",\".\")\n",
        "        hindi = hindi.replace(\"..\",\".\")\n",
        "        hindi = re.sub(r'\"\"','\"', str(hindi))\n",
        "        english = english.replace(\"....\",\".\")\n",
        "        english = english.replace(\"...\",\".\")\n",
        "        english = english.replace(\"..\",\".\")\n",
        "        # Remove multiple occurence of punctuations\n",
        "        if(\".\" in english and english.index(\".\")!=len(english)-1):\n",
        "            english = english.replace(\".\",\"\")\n",
        "        # Remove . if found in the middle of a sentence\n",
        "\n",
        "        if(len(english)==0 or len(hindi)==0): # Skip if sentence is blank\n",
        "            continue\n",
        "        flag=0\n",
        "        for i in range(65,123):# Find if there are english letters in the hindi sentence\n",
        "            if chr(i) in hindi:\n",
        "                flag=1\n",
        "                break\n",
        "        if flag==0:\n",
        "            dic[hindi.strip().lower()] = english.strip().lower() \n",
        "    for key, value in dic.items():\n",
        "        hin_len = 0\n",
        "        eng_len = 0\n",
        "        for token in tokenize_hindi(key):\n",
        "            hin_len+=1\n",
        "        for token in tokenize_english(value):\n",
        "            eng_len+=1\n",
        "        if hin_len<=50 and eng_len<=50:  # Accept the sentences if both their lengths<=50\n",
        "            writer.writerow([key, value])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNHaAJG3C_4F"
      },
      "source": [
        "The cleaned dataset is then shuffled to maintain randomness in the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuMOA4KWHPh3"
      },
      "source": [
        "# ip = open('cleaned_train.csv','r',encoding='utf-8')\n",
        "# ipdata = ip.readlines()\n",
        "# random.shuffle(ipdata)\n",
        "# with open('cleaned-randomized-train.csv','w',encoding='utf-8') as f:\n",
        "#     rows = '\\n'.join([row.strip() for row in ipdata])\n",
        "#     f.write(rows)\n",
        "train_data = []\n",
        "csv_file = open(\"cleaned-randomized-train.csv\",encoding='utf-8')\n",
        "rows = csv.reader(csv_file)\n",
        "for row in rows:\n",
        "    train_data.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y27uq5V3CYqf"
      },
      "source": [
        "# Splitting into train set and validation set in 85:15 ratio\n",
        "train_data_size=int(len(train_data)*0.85)\n",
        "train, validation = train_data[:train_data_size],train_data[train_data_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgFGCfEYt8o9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238f9e0f-375d-41df-a2dd-242fff8b9a18"
      },
      "source": [
        "while(len(train)%64>0): # Making sure that the length of train set is divisible by our batch size (64)\n",
        "    del train[-1]\n",
        "print(len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsa3eRTR63ii"
      },
      "source": [
        "## Creating vocabularies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VoxvACH64sA"
      },
      "source": [
        "Here, we create vocabularies for hindi and english words using 2 dictionaries for each - one for word-to-index conversion and second for index-to-word conversion. The indexes will be used for tensor representation of the sentences. We also keep track of the vocabulary size which will be used later for input and output sizes for encoder/decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz1MKnj20U--"
      },
      "source": [
        "sos = \"<sos>\" # All sentences will have <sos> at the start\n",
        "eos = \"<eos>\" # All sentences will be appended with <eos> at the end\n",
        "unk = \"\" # unk token is kept blank to prevent occurences of <unk> in the predictions\n",
        "pad = \"<pad>\"\n",
        "Eng_cnt = {} # Count of distinct english tokens found\n",
        "Hin_cnt = {} # Count of distinct hindi tokens found\n",
        "\n",
        "Eng_ind_word={0:sos, \n",
        "             1:eos,\n",
        "             2:unk, \n",
        "             3:pad\n",
        "} # Index to word translation for english tokens\n",
        "Eng_word_ind = {sos:0,\n",
        "                eos:1,\n",
        "                unk:2,\n",
        "                pad:3\n",
        "} # Word to index translation of english tokens\n",
        "Hin_ind_word = {0:sos, \n",
        "                1:eos,\n",
        "                2:unk, \n",
        "                3:pad\n",
        "} # Index to word translation of hindi tokens\n",
        "Hin_word_ind = {sos:0,\n",
        "                eos:1,\n",
        "                unk:2,\n",
        "                pad:3\n",
        "}# Word to index translation of hindi tokens\n",
        "eng_vocab_size = 4\n",
        "hin_vocab_size = 4\n",
        "# Above two variables keep track of our current vocab size\n",
        "\n",
        "eng_words = set()\n",
        "hin_words = set()\n",
        "# Sets of english and hindi words seen so far\n",
        "\n",
        "for row in train: # Each row contains a hindi, english sentence pair\n",
        "    english_tokens= tokenize_english(row[1])\n",
        "    for token in english_tokens:\n",
        "        if token not in eng_words: # New token found\n",
        "            Eng_word_ind[token]= eng_vocab_size\n",
        "            Eng_ind_word[eng_vocab_size]=token\n",
        "            Eng_cnt[token]=1\n",
        "            eng_vocab_size+=1\n",
        "            eng_words.add(token)\n",
        "        else: # Token already exists\n",
        "            Eng_cnt[token]+=1\n",
        "\n",
        "    hindi_tokens= tokenize_hindi(row[0]) # Do the same for hindi tokens\n",
        "    for token in hindi_tokens:\n",
        "        if token not in hin_words: # New token found\n",
        "            Hin_word_ind[token]= hin_vocab_size\n",
        "            Hin_ind_word[hin_vocab_size]=token\n",
        "            Hin_cnt[token]=1\n",
        "            hin_vocab_size+=1\n",
        "            hin_words.add(token)\n",
        "        else: # Token already exists\n",
        "            Hin_cnt[token]+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4HNOLvPJQMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0721257d-1ef0-4a6e-bdfc-909a972114d5"
      },
      "source": [
        "print(hin_vocab_size, eng_vocab_size) # Vocabulary sizes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37208 28157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJPcKp2M67bR"
      },
      "source": [
        "## Defining our architecture:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZUfle6UD12u"
      },
      "source": [
        "###Encoder: Single layer bidirectional GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APcgFTJPW6EB"
      },
      "source": [
        "In our encoder, we use a bidirectional RNN. Using this biRNN, we're able to process hindi sequences in the forward as well as in the backward direction parallely. This architecture hence gives us two context vectors, one capturing past information and another capturing future information. However, we need to concatenate these into a single context vector as our decoder is unidirectional. The concatenation is done by first concatenating the top layer forward RNN hidden state after the final time-step and top layer backward RNN hidden state after the final time-step horizontally, and then passing the resultant vector over a linear layer and then applying tanh activation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-gIxrq0FjiT"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, layers, dropout):\n",
        "        super(Encoder, self).__init__()   \n",
        "        self.embedding = nn.Embedding(input_size, embedding_size) # Defines our embedding\n",
        "        self.layers = layers\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, bidirectional = True)\n",
        "        # Bidirectional GRU used: A forward and a backward RNN\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_layer = nn.Linear(hidden_size * 2, hidden_size) # Defining a fully connected linear layer\n",
        "        \n",
        "    def forward(self, source_tensor): # source_tensor = hindi_tensors (indexes of hindi words from the vocab)\n",
        "        # let x = source tensor length. batch_size = 64, embedding_size = 512, hidden_size = 1024\n",
        "        # shape of source_tensor = [x, 64] where \n",
        "        source_embedding = self.dropout(self.embedding(source_tensor)) # 3D embedded input tensor\n",
        "        \n",
        "        outputs, hidden_state = self.gru(source_embedding) # Get the outputs from the RNN. No cell state in GRU.\n",
        "        \n",
        "        # [0, :, :] is the top layer forward RNN hidden state after the final time-step\n",
        "        # [1, :, :] gives the top layer backward RNN hidden state after the final time-step\n",
        "        hidden_cat = torch.cat((hidden_state[0,:,:], hidden_state[1,:,:]), dim = 1)\n",
        "        # Concatenating hidden layers of forward direction and backward direction because the decoder is not bidirectional\n",
        "        hidden_cat_fc = self.fc_layer(hidden_cat) # Passing through the fully connected layer\n",
        "        hidden_state = torch.tanh(hidden_cat_fc) #  Applying tanh activation\n",
        "        \n",
        "        return outputs, hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0tloo8Jj-n"
      },
      "source": [
        "### Decoder: Unidirectional single layer GRU with attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze1zoEh_1UxE"
      },
      "source": [
        "Our decoder uses an attention layer. Unlike a simple Seq2Seq-RNN model, here we use attention-weighted source vectors at every time step while decoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2HyNHN7P70r"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_size, hidden_size, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_size = output_size  \n",
        "        self.embedding = nn.Embedding(output_size, embedding_size) # Defines our embedding\n",
        "        self.gru = nn.GRU((hidden_size * 2) + embedding_size, hidden_size) # Unidirectional GRU\n",
        "        self.fc_layer_out = nn.Linear((hidden_size * 3) + embedding_size, output_size) # Defining a fully connected linear layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attention_layer = nn.Linear((hidden_size * 3), hidden_size)\n",
        "        self.weighted_energy_sum = nn.Linear(hidden_size, 1, bias = False) # Represents weighted sum of the energy across all encoder hidden states. \n",
        "\n",
        "    def forward(self, decoder_input, hidden_state, encoder_outputs):\n",
        "        # let x = source tensor length. batch_size = 64, embedding_size = 512, hidden_size = 1024\n",
        "\n",
        "        enc_op_copy = encoder_outputs\n",
        "        hid_state_copy = hidden_state # Making copies of these for the attention mechanism\n",
        "        \n",
        "        decoder_input = decoder_input.unsqueeze(0) # Add 1 dimension\n",
        "        \n",
        "        # Before squeeze: decoder_input = [64]. After squeeze: [1, 64]\n",
        "        \n",
        "        source_embedding = self.dropout(self.embedding(decoder_input)) # Apply dropout on the source embedding\n",
        "\n",
        "        source_length =  enc_op_copy.shape[0]\n",
        "        batch_size =  enc_op_copy.shape[1]\n",
        "        \n",
        "        #repeat decoder hidden state source_length times\n",
        "        hid_state_copy = hid_state_copy.unsqueeze(1).repeat(1, source_length, 1) # Repeating the previous decoder hidden state source_length times\n",
        "\n",
        "        enc_op_copy =  enc_op_copy.permute(1, 0, 2) # Switch dimensions \n",
        "\n",
        "        concat = self.attention_layer(torch.cat((hid_state_copy, enc_op_copy), dim = 2)) # Concatenating encoder outputs and hidden state\n",
        "        energy = torch.tanh(concat) # Represents the energy between the previous decoder hidden state and the encoder hidden states.\n",
        "        \n",
        "        # Now we compute our attention vector\n",
        "        attention_vector = self.weighted_energy_sum(energy).squeeze(2) # energy gets multiplied by a [1, hidden_size] tensor weighted_energy_sum.\n",
        "        # weighted_energy_sum represents the weighted sum of the energy across all encoder hidden states        \n",
        "        attention_vector = func.softmax(attention_vector, dim=1).unsqueeze(1) # Softmax ensures that the elements of the vector are between 0 and 1\n",
        "        # This is our final attention vector over the hindi sentences\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # Swap dimensions\n",
        "        \n",
        "        attn_weighted_encoder_outputs = torch.bmm(attention_vector, encoder_outputs) # Performs a batch matrix-matrix product of matrices stored in attention_vector and encoder_outputs\n",
        "        attn_weighted_encoder_outputs = attn_weighted_encoder_outputs.permute(1,0,2)\n",
        "        # Created a a weighted sum of the encoder outputs\n",
        "        \n",
        "        gru_input = torch.cat((source_embedding, attn_weighted_encoder_outputs), dim = 2) # [1, batch size, (hidden_size*2) + embedding_size]\n",
        "        decoder_output, hidden_state = self.gru(gru_input, hidden_state.unsqueeze(0))\n",
        "        # Get the prediction from the RNN. decoder_output shape = [x, 64, 1024*2] where x = sentence length, 64 = batch size and 1024 = hidden size\n",
        "        # hidden_state shape = [2, 64, 1024] where 2 = number of directions\n",
        "        \n",
        "        # Pass the embedded input, weighted source tensor and decoder output from the RNN over a linear layer to predict the next word of the english sentence\n",
        "        decoder_prediction = self.fc_layer_out(torch.cat((decoder_output.squeeze(0), attn_weighted_encoder_outputs.squeeze(0), source_embedding.squeeze(0)), dim = 1))\n",
        "        hidden_state = hidden_state.squeeze(0) # Remove extra dimension\n",
        "        return decoder_prediction, hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftxg_QwcgQPF"
      },
      "source": [
        "### Combining the encoder and decoder in our Seq2seq class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl1UJMOaDCly"
      },
      "source": [
        "class Seq2seq(nn.Module): # Defines our model. Combines the encoder and the decoder\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2seq, self).__init__()      \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, source_tensor, target_tensor, teacher_force_ratio = 0.5):\n",
        "        # For \"good\" predictions, sometimes we use the correct word, sometimes (50% of the times) we use the predicted word. If the ratio is 1, it might cause overfitting. If it is 0 (only predicted words taken), it might underfit because a wrong prediction can ruin the prediction of a lot of subsequent words\n",
        "   \n",
        "        batch_size = source_tensor.shape[1]\n",
        "        target_length = target_tensor.shape[0]\n",
        "        \n",
        "        decoder_outputs = torch.zeros(target_length, batch_size, eng_vocab_size).to(device)\n",
        "        # We're going to predict 1 word at a time for an entire batch, and each prediction is going to be a vector of entire english vocab size\n",
        "\n",
        "        encoder_outputs, hidden_state = self.encoder(source_tensor) # Get the encoder outputs for the source tensor \n",
        "        decoder_input = target_tensor[0,:] # Append the <sos> tokens\n",
        "        \n",
        "        for i in range(1, target_length):\n",
        "            \n",
        "            decoder_output, hidden_state = self.decoder(decoder_input, hidden_state, encoder_outputs) # inputs given are from the encoder\n",
        "            # The output from this is going to be the decoder output and the next hidden_state which will be reused in the loop\n",
        "            # Size of decoder_output = (batch_size, target_vocab_size) ie (N, eng_vocab_size)\n",
        "            decoder_outputs[i] = decoder_output\n",
        "            \n",
        "            if random.random() < teacher_force_ratio: # If this condition is satisfied, use the actual next word (ground truth) as the next input to decoder\n",
        "                decoder_input = target_tensor[i]\n",
        "            else: # else use the actual prediction as the next input to decoder\n",
        "                decoder_input = decoder_output.argmax(1) # Taking argmax of the 1st dimension to get the highest predicted token\n",
        "\n",
        "        return decoder_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C_Q6vNHGi5Y"
      },
      "source": [
        "## Preparing tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWX8lMiwGmXj"
      },
      "source": [
        "The idea is to create tensors of train data and iterate over them in steps of batch size. We use the DataLoader module of pytorch to do this. Now, DataLoader needs all the tensors in a batch to be of equal length, so we need to pad the tensors to make them of equal size. Instead of having a global maximum tensor length, we use a dictionary to keep track of the maximum length of tensors in each batch. This saves computation time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd8wl_peh0Q8"
      },
      "source": [
        "batch_maxlen={} # Dictionary to store max length of tensors in each batch\n",
        "batch_size = 64 # Batch size 64 found to be optimal\n",
        "batch_no=1\n",
        "for i in range(0,len(train),batch_size): # Iterate over the train set in steps of batch size.\n",
        "    max_sentence_len = 0 \n",
        "    for row in train[i:i+batch_size]: # This represents rows in our current batch\n",
        "        hindi = row[0]\n",
        "        english = row[1]\n",
        "        eng_maxlen, hin_maxlen = 0,0 \n",
        "        for token in tokenize_hindi(hindi):\n",
        "            hin_maxlen+=1 # Count hindi tokens\n",
        "        for token in tokenize_english(english):\n",
        "            eng_maxlen+=1 # Count english tokens\n",
        "        max_sentence_len = max(max_sentence_len,max(hin_maxlen, eng_maxlen)) # Max sentence length is the max over hindi and english sentences in the current batch\n",
        "    batch_maxlen[batch_no] = max_sentence_len+2 # +2 done to account for <sos> and <eos> tokens\n",
        "    batch_no+=1 # Go to the next batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELsUjLYQueU5"
      },
      "source": [
        "train_tensors = [] # Our cumulative train tensors. A list of list containing index vectors for all the sentences in the train corpus\n",
        "batch_no=1\n",
        "for i in range(0,len(train),batch_size): # Iterate over the train set in steps of batch size.\n",
        "    max_sentence_len=batch_maxlen[batch_no] # Find the max sentence length in this batch\n",
        "    for row in train[i:i+batch_size]: # This represents rows in our current batch\n",
        "        hindi = row[0]\n",
        "        english = row[1]\n",
        "        hindi_indexes = [] # To store indices for hindi tokens\n",
        "        eng_indexes = [] # To store indices for english tokens\n",
        "        for token in tokenize_hindi(hindi):\n",
        "            if Hin_word_ind.get(token)==None: # Token not found in vocabulary\n",
        "                hindi_indexes.append(Hin_word_ind[unk]) # 2 is for unk token\n",
        "            else:\n",
        "                hindi_indexes.append(Hin_word_ind.get(token)) # Append the index of the current token\n",
        "        hindi_indexes.insert(0,Hin_word_ind[sos]) # Insert the <sos> token's index at start\n",
        "        hindi_indexes.append(Hin_word_ind[eos]) # Append the <eos> token's index at end\n",
        "        while(len(hindi_indexes) < max_sentence_len):\n",
        "            hindi_indexes.append(Hin_word_ind[pad]) # Padding\n",
        "        for token in tokenize_english(english):\n",
        "            if Eng_word_ind.get(token)==None: # Token not found in vocabulary\n",
        "                eng_indexes.append(Eng_word_ind[unk]) # 2 is for unk token\n",
        "            else:\n",
        "                eng_indexes.append(Eng_word_ind.get(token)) # Append the index of the current token\n",
        "        eng_indexes.insert(0,Eng_word_ind[sos])# Insert the <sos> token's index at start\n",
        "        eng_indexes.append(Eng_word_ind[eos])# Append the <eos> token's index at end\n",
        "        while(len(eng_indexes) < max_sentence_len):\n",
        "            eng_indexes.append(Eng_word_ind[pad]) # Padding\n",
        "\n",
        "        train_tensors+=[[torch.IntTensor(hindi_indexes).detach().clone(), torch.IntTensor(eng_indexes).detach().clone()]]\n",
        "    batch_no+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z51TtUjv6-KD"
      },
      "source": [
        "## Some prerequisites before we start training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1557A3ghDN-"
      },
      "source": [
        "# Setting our hyperparameters\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "layers = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = hin_vocab_size\n",
        "output_size = eng_vocab_size\n",
        "hidden_size = 512 # Hidden size taken the same for both encoder and decoder\n",
        "encoder_embedding_size = 256\n",
        "decoder_embedding_size = 256\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWaqNsHBe6qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0de4c1-d1db-4f62-8987-02276a55e8f6"
      },
      "source": [
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, layers, encoder_dropout)\n",
        "decoder_net = Decoder(output_size, decoder_embedding_size, hidden_size, decoder_dropout)\n",
        "# Defined our encoder and decoder net\n",
        "\n",
        "model = Seq2seq(encoder_net, decoder_net).to(device) # Initialize our model and send to cuda\n",
        "pad_index = Eng_word_ind['<pad>'] \n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_index) # We don't want to pay loss for <pad>, so ignore pad indexes\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)  # AdamW optimizer used instead of plain old Adam.\n",
        "def init_weights(model): # Initializing weights\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(parameter.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(parameter.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(37208, 256)\n",
              "    (gru): GRU(256, 512, bidirectional=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (fc_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(28157, 256)\n",
              "    (gru): GRU(1280, 512)\n",
              "    (fc_layer_out): Linear(in_features=1792, out_features=28157, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (attention_layer): Linear(in_features=1536, out_features=512, bias=True)\n",
              "    (weighted_energy_sum): Linear(in_features=512, out_features=1, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weQgm6tDugCh"
      },
      "source": [
        "iterator = DataLoader(train_tensors, batch_size=batch_size,shuffle=False) # Created train iterator mimicking's torchtext's train iterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k28b1N1DuiKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad6c6ee-4cf4-470e-c21d-c01a346862f8"
      },
      "source": [
        "#torch.save(model.state_dict(), 'final-phase-45.pt')\n",
        "model.load_state_dict(torch.load('final-model.pt')) #loading the saved model\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(37208, 256)\n",
              "    (gru): GRU(256, 512, bidirectional=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (fc_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(28157, 256)\n",
              "    (gru): GRU(1280, 512)\n",
              "    (fc_layer_out): Linear(in_features=1792, out_features=28157, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (attention_layer): Linear(in_features=1536, out_features=512, bias=True)\n",
              "    (weighted_energy_sum): Linear(in_features=512, out_features=1, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFZT77OCMu_7"
      },
      "source": [
        "## Commence training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gimqflXQwCMH",
        "outputId": "c2182407-284b-4590-df1b-b6d8af96745a"
      },
      "source": [
        "total_loss = 0 # To keep track of the training loss in each epoch\n",
        "for epoch in range(31,epochs):\n",
        "    print(\"Epoch\"+str(epoch) +str(\"/\") + str(epochs))\n",
        "    model.train() # Setting model in train mode\n",
        "    for id, batch in enumerate(iterator): # Iterating over the training set in batches of 64\n",
        "\n",
        "        hindi_input=torch.transpose(batch[0].long(), 0, 1).to(device)\n",
        "        english_target=torch.transpose(batch[1].long(), 0, 1).to(device)\n",
        "        # hindi and english tensors have shape = (batch_size, max_batchlen) but we need shape to be (max_batchlen, batch_size ) so transpose these\n",
        "\n",
        "        output = model(hindi_input, english_target) # Forward propagation\n",
        "        output = output[1:].reshape(-1, output.shape[2]) # We're going to keep the output dimension, which is the size of the vocab, and put everything else together, and gotta discard the first output(<sos>)\n",
        "      \n",
        "        english_target = english_target[1:].reshape(-1) # Remove the <sos> token from the ground truth target\n",
        "        optimizer.zero_grad() \n",
        "        loss = criterion(output, english_target) # Calculate the CrossEntropy loss \n",
        "        loss.backward() # Back propagation\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # Clipping the gradients to prevent them from exploding\n",
        "        optimizer.step() # Gradient descent\n",
        "        total_loss+=loss.item() # Add the current batch loss to the total epoch loss\n",
        "\n",
        "    if epoch==35:\n",
        "        torch.save(model.state_dict(), 'attn-gru4-35.pt')\n",
        "    elif epoch==40:\n",
        "        torch.save(model.state_dict(), 'attn-gru4-40.pt')\n",
        "    elif epoch==45:\n",
        "        torch.save(model.state_dict(), 'attn-gru4-45.pt')  \n",
        "    elif epoch==50:\n",
        "        torch.save(model.state_dict(), 'attn-gru4-50.pt')\n",
        "    print(\"Train loss :\", total_loss/len(iterator))\n",
        "    total_loss = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 31 / 50]\n",
            "Training loss : 1.3845185347742808\n",
            "[Epoch 32 / 50]\n",
            "Training loss : 1.3792187408612715\n",
            "[Epoch 33 / 50]\n",
            "Training loss : 1.360443650407994\n",
            "[Epoch 34 / 50]\n",
            "Training loss : 1.3579538188472318\n",
            "[Epoch 35 / 50]\n",
            "Training loss : 1.3380614352792062\n",
            "[Epoch 36 / 50]\n",
            "Training loss : 1.3252727541517704\n",
            "[Epoch 37 / 50]\n",
            "Training loss : 1.3163236871288568\n",
            "[Epoch 38 / 50]\n",
            "Training loss : 1.299136302962826\n",
            "[Epoch 39 / 50]\n",
            "Training loss : 1.2950468673058306\n",
            "[Epoch 40 / 50]\n",
            "Training loss : 1.2800884007334514\n",
            "[Epoch 41 / 50]\n",
            "Training loss : 1.2639049985209776\n",
            "[Epoch 42 / 50]\n",
            "Training loss : 1.257107134341413\n",
            "[Epoch 43 / 50]\n",
            "Training loss : 1.251821501165287\n",
            "[Epoch 44 / 50]\n",
            "Training loss : 1.2385139078048168\n",
            "[Epoch 45 / 50]\n",
            "Training loss : 1.2211567567335213\n",
            "[Epoch 46 / 50]\n",
            "Training loss : 1.2208030289206684\n",
            "[Epoch 47 / 50]\n",
            "Training loss : 1.2092267169987512\n",
            "[Epoch 48 / 50]\n",
            "Training loss : 1.1998458169583603\n",
            "[Epoch 49 / 50]\n",
            "Training loss : 1.1923794133280772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98xRj312M1-T"
      },
      "source": [
        "## **Evaluation on validation set:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp5s_wSHCew"
      },
      "source": [
        "def translate(model, sentence, max_length): # Function for translating hindi to english using our trained model\n",
        "\n",
        "    tokens = [t for t in tokenize_hindi(sentence)] # Extract tokens from the current hindi sentence\n",
        "\n",
        "    tokens.insert(0, \"<sos>\")\n",
        "    tokens.append(\"<eos>\")\n",
        "    # Add <sos> and <eos> tokens in beginning and end respectively\n",
        "\n",
        "    text_to_indices = [] # Stores the indices of hindi tokens\n",
        "    for token in tokens: # Go through each hindi token and convert to its index in the vocab\n",
        "        if Hin_word_ind.get(token)==None:\n",
        "            text_to_indices.append(2) # 2 is for unk token\n",
        "        else:\n",
        "            text_to_indices.append(Hin_word_ind[token])\n",
        "    sentence_tensor = torch.IntTensor(text_to_indices)\n",
        "    sentence_tensor = sentence_tensor.unsqueeze(1).to(device)\n",
        "     # Convert to Tensor and send to cuda\n",
        "     \n",
        "    with torch.no_grad():\n",
        "        encoder_states, hidden = model.encoder(sentence_tensor) # Fetch encoder outputs\n",
        "\n",
        "    decoder_outputs = [Eng_word_ind[\"<sos>\"]] # decoder_outputs stores our english translations\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.IntTensor([decoder_outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(previous_word, hidden, encoder_states)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        decoder_outputs.append(best_guess)\n",
        "\n",
        "        if output.argmax(1).item() == Eng_word_ind[\"<eos>\"]:\n",
        "            break\n",
        "    translated_sentence = [] # Stores the word conversions of target sentence\n",
        "    for idx in decoder_outputs:\n",
        "        translated_sentence.append(Eng_ind_word[int(idx)]) # Convert indices to english words \n",
        "\n",
        "    if translated_sentence[-1] == \"<eos>\":\n",
        "        del translated_sentence[-1] # Remove <eos> token if found at end\n",
        "    return \" \".join(translated_sentence[1:]) # Remove <sos> token and return the english prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCK5ki-KQTA_"
      },
      "source": [
        "Generating ground truth english translations and predicted english translations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fLliHsmM-rr"
      },
      "source": [
        "file = open(\"ground_truth.txt\",\"w\") # This txt will store the groud truth english translations\n",
        "for row in validation:\n",
        "    file.write(row[1]+\"\\n\")\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTZQW_S5gn2F"
      },
      "source": [
        "Generating model predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_y8aBQwNASG"
      },
      "source": [
        "model.eval()\n",
        "file1 = open(\"model-prediction.txt\",\"w\") # Stores our model predictions\n",
        "for row in validation:\n",
        "    sentence = row[0]\n",
        "    translated_sentence = translate( # Translated english sentence\n",
        "        model, sentence, max_length=50\n",
        "    )\n",
        "    file1.write(translated_sentence + \"\\n\")\n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq9Y1Cc6NFsT"
      },
      "source": [
        "## **Generating predictions on test set:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ73-sEJQAW3"
      },
      "source": [
        "### We preprocess the hindi statements in test dataset in the same way we preprocessed hindi statements in the train dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCStby1HMY2m"
      },
      "source": [
        "with open('testhindistatements-cleaned.csv', 'w', newline='\\n',encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    with open('testhindistatements.csv',encoding='utf-8') as hfile:\n",
        "        rows = csv.reader(hfile)\n",
        "        for row in rows:\n",
        "            hindi = row[2]\n",
        "            hindi = hindi.replace('’','\\'')\n",
        "            hindi=re.sub(r'[/\\-:;%()♪♫<>,~¶#&=]+','',hindi)\n",
        "            hindi=re.sub(' +',' ', hindi)\n",
        "            if(\"...\" in hindi and hindi.index(\"...\")!=len(hindi)-3):\n",
        "                hindi = hindi.replace(\"...\",\" \")\n",
        "            hindi = hindi.replace(\"....\",\".\")\n",
        "            hindi = hindi.replace(\"...\",\".\")\n",
        "            hindi = hindi.replace(\"..\",\".\")\n",
        "            hindi = hindi.replace(\"०\", \"0\")\n",
        "            hindi = hindi.replace(\"१\", \"1\")\n",
        "            hindi = hindi.replace(\"२\", \"2\")\n",
        "            hindi = hindi.replace(\"३\", \"3\")\n",
        "            hindi = hindi.replace(\"४\", \"4\")\n",
        "            hindi = hindi.replace(\"५\", \"5\")\n",
        "            hindi = hindi.replace(\"६\", \"6\")\n",
        "            hindi = hindi.replace(\"७\", \"7\")\n",
        "            hindi = hindi.replace(\"८\", \"8\")\n",
        "            hindi = hindi.replace(\"९\", \"9\")\n",
        "            hindi=re.sub('\"+','', hindi)\n",
        "            hindi=re.sub(' +',' ', hindi)\n",
        "            writer.writerow([row[0],row[1],hindi])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwY0MX38MZUj"
      },
      "source": [
        "model.eval()\n",
        "file2 = open(\"testhindistatements.txt\",\"w\")\n",
        "file3 = open(\"answer.txt\",\"w\")\n",
        "csv_file2 = open('testhindistatements-cleaned.csv',encoding='utf-8')\n",
        "rows = csv.reader(csv_file2)\n",
        "cnt=0\n",
        "for row in rows:\n",
        "    if cnt==0: # Skip the 0th row\n",
        "        cnt+=1\n",
        "        continue\n",
        "    sentence = row[2]\n",
        "    translated_sentence = translate(\n",
        "        model, sentence, max_length=80\n",
        "    )\n",
        "    file2.write(sentence + \"\\n\")\n",
        "    file3.write(translated_sentence + \"\\n\")\n",
        "file2.close()\n",
        "file3.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryuKURz0dxal"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdB0o17kdzpL"
      },
      "source": [
        "[1] https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "[2] https://arxiv.org/pdf/1409.0473.pdf\n",
        "\n",
        "[3] Text preprocessing tutorial: https://colab.research.google.com/drive/1p3oGPcNdORw5_MDcufTDYWJhJt3XVPuC?usp=sharing"
      ]
    }
  ]
}